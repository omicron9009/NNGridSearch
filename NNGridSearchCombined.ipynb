{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0503b058",
   "metadata": {},
   "source": [
    "# 🧠 AutoML Neural Network Search (Phase 2)\n",
    "\n",
    "Welcome to the **AutoML Neural Network Architecture Search — Phase 2**.\n",
    "\n",
    "This project builds a fully automated deep learning architecture search system designed to:\n",
    "\n",
    "- 🔍 Automatically explore CNN + AutoEncoder structures.\n",
    "- ⚙️ Handle flexible hyperparameter combinations.\n",
    "- 🔢 Search across depth (layers) and width (filters, kernels, dense units, latent space).\n",
    "- 🧮 Extract and print filters from trained convolutional layers.\n",
    "- 🖼️ Visualize filters layer-wise (optional).\n",
    "- 📊 Modular design: easy to extend for any image dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Features\n",
    "\n",
    "- **AutoML Search Engine**\n",
    "  - Supports both `grid` and `random` search.\n",
    "  - Random search uses internal clipping for faster execution.\n",
    "  \n",
    "- **Dynamic Model Builder**\n",
    "  - Builds CNN AutoEncoders dynamically based on search parameters.\n",
    "\n",
    "- **Filter Extraction**\n",
    "  - Extracts weights (filters) from convolutional layers after training.\n",
    "\n",
    "- **Filter Visualization**\n",
    "  - Prints or plots filters to understand layer-wise feature extraction.\n",
    "\n",
    "- **Completely Modular**\n",
    "  - Simple file structure for easy extension and testing.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Current Phase: Phase 2 PRO Version\n",
    "\n",
    "- Multi-layer CNN AutoEncoders\n",
    "- Fully parameterized search space\n",
    "- Multiple convolutional layers handled automatically\n",
    "- Clean filter extraction per layer\n",
    "- Streamlined random search with sample clipping\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Note\n",
    "\n",
    "> This project is intentionally simple, interpretable and fully transparent AutoML framework — perfect for:\n",
    "> \n",
    "> - Learning\n",
    "> - Experimentation\n",
    "> - Extension to real-world datasets\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Next Steps\n",
    "\n",
    "- Build more advanced optimization (Phase 3)\n",
    "- Add dataset auto-adaptation\n",
    "- Implement scoring customization\n",
    "- More efficient search strategies\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a87f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoML_Demo_Template_V2.py\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from automl_search import AutoMLSearch\n",
    "from cnn_autoencoder_builder_v2 import cnn_autoencoder_builder_v2\n",
    "from get_plot_filters import plot_all_filters, list_conv_layers\n",
    "\n",
    "# 1️⃣ Load and prepare dataset (replaceable with any dataset)\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "x = np.concatenate((x_train, x_test), axis=0)\n",
    "x = x.astype('float32') / 255.0\n",
    "x = np.reshape(x, (x.shape[0], 28, 28, 1))\n",
    "X_train, X_val = train_test_split(x, test_size=0.2, random_state=42)\n",
    "\n",
    "# Optional: subset to speed up testing\n",
    "X_train, X_val = X_train[:5000], X_val[:1000]\n",
    "\n",
    "# 2️⃣ Define parameter grid (fully flexible for your search space)\n",
    "param_grid = {\n",
    "    'num_conv_layers': [1, 2],\n",
    "    'conv_filters_list': [[16], [16, 32]],\n",
    "    'kernel_sizes_list': [[(3, 3)], [(3, 3), (3, 3)]],\n",
    "    'pooling_type': ['max'],\n",
    "    'dropout_rate': [0.2],\n",
    "    'dense_units': [32],\n",
    "    'activation': ['relu'],\n",
    "    'latent_dim': [8]\n",
    "}\n",
    "\n",
    "# 3️⃣ Build and initialize AutoML search\n",
    "search = AutoMLSearch(\n",
    "    model_builder=cnn_autoencoder_builder_v2,\n",
    "    param_grid=param_grid,\n",
    "    mode='random',  # or 'grid'\n",
    "    scoring='val_loss',\n",
    "    n_jobs=1,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "# 4️⃣ Run the search\n",
    "search.fit(X_train, X_val)\n",
    "\n",
    "# 5️⃣ Show best parameters\n",
    "print(\"\\n✅ Best Hyperparameters Found:\")\n",
    "print(search.get_best_params())\n",
    "\n",
    "# 6️⃣ Extract filters from best model\n",
    "filters = search.extract_filters()\n",
    "\n",
    "# 7️⃣ List available conv layers\n",
    "list_conv_layers(search.get_best_model())\n",
    "\n",
    "# 8️⃣ Plot filters for inspection\n",
    "plot_all_filters(filters)\n",
    "\n",
    "print(\"\\n🎯 AutoML V2 PRO Demo Run Completed Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd17a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import multiprocessing\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class AutoMLSearch:\n",
    "\n",
    "    def __init__(self, model_builder, param_grid, input_shape, mode='grid', n_jobs=1, epochs=10, scoring='val_loss'):\n",
    "        self.model_builder = model_builder\n",
    "        self.param_grid = param_grid\n",
    "        self.input_shape = input_shape\n",
    "        self.mode = mode\n",
    "        self.n_jobs = n_jobs\n",
    "        self.epochs = epochs\n",
    "        self.scoring = scoring\n",
    "        self.results = []\n",
    "\n",
    "    def _param_combinations(self):\n",
    "        keys = list(self.param_grid.keys())\n",
    "        values = list(self.param_grid.values())\n",
    "\n",
    "        # Convert nested lists to tuples for cartesian product\n",
    "        processed_values = []\n",
    "        for v in values:\n",
    "            if isinstance(v[0], list): \n",
    "                processed_values.append([tuple(i) for i in v])\n",
    "            else:\n",
    "                processed_values.append(v)\n",
    "\n",
    "        combos = list(itertools.product(*processed_values))\n",
    "\n",
    "        for combo in combos:\n",
    "            params = dict(zip(keys, combo))\n",
    "\n",
    "            # Convert back to list where required\n",
    "            if 'conv_filters_list' in params:\n",
    "                params['conv_filters_list'] = list(params['conv_filters_list'])\n",
    "            if 'kernel_sizes_list' in params:\n",
    "                params['kernel_sizes_list'] = list(params['kernel_sizes_list'])\n",
    "            yield params\n",
    "\n",
    "    def fit(self, X_train, X_val):\n",
    "        combinations = list(self._param_combinations())\n",
    "\n",
    "        if self.mode == 'random':\n",
    "            sample_size = min(20, len(combinations))  # Clip to 20 samples\n",
    "            combinations = random.sample(combinations, sample_size)\n",
    "\n",
    "        for params in combinations:\n",
    "            try:\n",
    "                model = self.model_builder(self.input_shape, params)\n",
    "                history = model.fit(X_train, X_train, validation_data=(X_val, X_val), epochs=self.epochs, verbose=0)\n",
    "                score = history.history[self.scoring][-1]\n",
    "                self.results.append({'params': params, 'score': score, 'model': model})\n",
    "                print(f\"Params: {params} | Score: {score:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed on params {params}: {e}\")\n",
    "\n",
    "        self.results.sort(key=lambda x: x['score'])\n",
    "        self.best_model = self.results[0]['model'] if self.results else None\n",
    "        self.best_params = self.results[0]['params'] if self.results else None\n",
    "\n",
    "    def get_best_model(self):\n",
    "        return self.best_model\n",
    "\n",
    "    def get_best_params(self):\n",
    "        return self.best_params\n",
    "\n",
    "    def extract_filters(self):\n",
    "        filters = []\n",
    "        for layer in self.best_model.layers:\n",
    "            if 'conv2d' in layer.name.lower():\n",
    "                filters.append(layer.get_weights()[0])\n",
    "        return filters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b4092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, Flatten, Dense, Reshape\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def cnn_autoencoder_builder(input_shape, params):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    # Encoder\n",
    "    for i in range(params['num_conv_layers']):\n",
    "        filters = params['conv_filters_list'][i]\n",
    "        kernel_size = params['kernel_sizes_list'][i]\n",
    "        x = layers.Conv2D(filters, kernel_size, activation=params['activation'], padding='same')(x)\n",
    "\n",
    "        if params['pooling_type'] == 'max':\n",
    "            x = layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "        elif params['pooling_type'] == 'avg':\n",
    "            x = layers.AveragePooling2D(pool_size=(2,2))(x)\n",
    "\n",
    "        if params['dropout_rate'] > 0:\n",
    "            x = layers.Dropout(params['dropout_rate'])(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(params['dense_units'], activation=params['activation'])(x)\n",
    "    encoded = layers.Dense(params['latent_dim'], activation=params['activation'])(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = layers.Dense(params['dense_units'], activation=params['activation'])(encoded)\n",
    "    x = layers.Dense(input_shape[0]*input_shape[1]*input_shape[2], activation='sigmoid')(x)\n",
    "    decoded = layers.Reshape(input_shape)(x)\n",
    "\n",
    "    model = models.Model(inputs, decoded)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b4075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_plot_filters.py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def plot_layer_filters(model=None, filters=None, layer_name=None, layer_index=None):\n",
    "    \"\"\"\n",
    "    Plots filters of a convolutional layer.\n",
    "\n",
    "    Args:\n",
    "        model: Trained Keras model (optional if filters provided).\n",
    "        filters: Tuple of (filters, biases) directly from extract_filters().\n",
    "        layer_name: Name of layer (if model provided).\n",
    "        layer_index: Index of layer (if model provided).\n",
    "    \"\"\"\n",
    "    # Determine how filters are provided\n",
    "    if filters:\n",
    "        filters, biases = filters\n",
    "    elif model:\n",
    "        if layer_name:\n",
    "            layer = model.get_layer(name=layer_name)\n",
    "        elif layer_index is not None:\n",
    "            layer = model.layers[layer_index]\n",
    "        else:\n",
    "            raise ValueError(\"Provide either layer_name or layer_index\")\n",
    "\n",
    "        try:\n",
    "            filters, biases = layer.get_weights()\n",
    "        except:\n",
    "            raise ValueError(\"Selected layer has no filters.\")\n",
    "\n",
    "    num_filters = filters.shape[-1]\n",
    "    kernel_shape = filters.shape[:3]\n",
    "\n",
    "    cols = 8\n",
    "    rows = math.ceil(num_filters / cols)\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i in range(num_filters):\n",
    "        f = filters[:, :, :, i]\n",
    "        if f.shape[-1] == 1:\n",
    "            f = f[:, :, 0]\n",
    "        axs[i].imshow(f, cmap='viridis')\n",
    "        axs[i].set_title(f'Filter {i+1}', fontsize=8)\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        axs[j].axis('off')\n",
    "\n",
    "    plt.suptitle(f\"Filters | Shape: {kernel_shape} | Total Filters: {num_filters}\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def list_conv_layers(model):\n",
    "    \"\"\"\n",
    "    Utility to list all Conv2D layers in model.\n",
    "    \"\"\"\n",
    "    print(\"Available Conv2D layers:\")\n",
    "    for idx, layer in enumerate(model.layers):\n",
    "        if 'conv2d' in layer.name.lower():\n",
    "            print(f\"Index: {idx}, Name: {layer.name}, Filters: {layer.filters if hasattr(layer, 'filters') else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23796f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phase2_output_extractor.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def extract_and_print_filters(model):\n",
    "    \"\"\"\n",
    "    Extracts and prints filter shapes and weights layer-wise for all Conv2D layers.\n",
    "\n",
    "    Args:\n",
    "        model: Trained Keras model.\n",
    "    \"\"\"\n",
    "    print(\"\\n Extracting Filters from Model...\\n\")\n",
    "    \n",
    "    for idx, layer in enumerate(model.layers):\n",
    "        if 'conv2d' in layer.name.lower():\n",
    "            try:\n",
    "                filters, biases = layer.get_weights()\n",
    "                shape = filters.shape  # (3, 3, in_channels, out_channels)\n",
    "                \n",
    "                print(f\"Layer {idx} - {layer.name}\")\n",
    "                print(f\"  Filter Shape: {shape}\")\n",
    "                \n",
    "                # Print first filter as example\n",
    "                print(f\"\\n  Filter 1 weights (kernel values):\\n\")\n",
    "                print(np.round(filters[:, :, :, 0], 4))  # only print 1st filter\n",
    "\n",
    "                print(\"-\" * 50)\n",
    "            except:\n",
    "                print(f\" No filters found in layer: {layer.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4331842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from automl_search import AutoMLSearch\n",
    "from cnn_autoencoder_builder import cnn_autoencoder_builder\n",
    "from visualize_filters import print_filters\n",
    "from get_plot_filters import plot_layer_filters, list_conv_layers\n",
    "\n",
    "\n",
    "# Load data\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "x = np.concatenate((x_train, x_test), axis=0)\n",
    "x = x.astype('float32') / 255.0\n",
    "x = np.reshape(x, (x.shape[0], 28, 28, 1))\n",
    "X_train, X_val = train_test_split(x, test_size=0.2, random_state=42)\n",
    "X_train, X_val = X_train[:10000], X_val[:2000]\n",
    "\n",
    "# Param grid\n",
    "param_grid = {\n",
    "    'num_conv_layers': [2, 3],\n",
    "    'conv_filters_list': [\n",
    "        [8],\n",
    "        [8, 16],\n",
    "        [8, 16, 32]\n",
    "    ],\n",
    "    'kernel_sizes_list': [\n",
    "        [(3,3)],\n",
    "        [(3,3), (3,3)],\n",
    "        [(3,3), (3,3), (3,3)]\n",
    "    ],\n",
    "    'pooling_type': ['max', 'avg'],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'dense_units': [16, 32, 64],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'latent_dim': [4, 8, 16]\n",
    "}\n",
    "\n",
    "\n",
    "# Search\n",
    "search = AutoMLSearch(\n",
    "    model_builder=cnn_autoencoder_builder,\n",
    "    param_grid=param_grid,\n",
    "    input_shape=(28,28,1),\n",
    "    mode='random',\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "search.fit(X_train, X_val)\n",
    "\n",
    "# Best params\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(search.get_best_params())\n",
    "\n",
    "# Filters\n",
    "filters = search.extract_filters()\n",
    "print(\"Filters\")\n",
    "print(\"*\"*50)\n",
    "print_filters(filters)\n",
    "print(\"*\"*50)\n",
    "best_model = search.get_best_model()\n",
    "# list_conv_layers(best_model)\n",
    "from output_extractor import extract_and_print_filters\n",
    "extract_and_print_filters(best_model)\n",
    "plot_layer_filters(model=best_model, layer_index=1)\n",
    "\n",
    "# from output_extractor import extract_and_print_filters\n",
    "# extract_and_print_filters(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2779e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from automl_search import AutoMLSearch\n",
    "from cnn_autoencoder_builder import cnn_autoencoder_builder\n",
    "from get_plot_filters import plot_layer_filters, list_conv_layers\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "x = np.concatenate((x_train, x_test), axis=0)\n",
    "x = x.astype('float32') / 255.0\n",
    "x = np.reshape(x, (x.shape[0], 28, 28, 1))\n",
    "X_train, X_val = train_test_split(x, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'num_conv_layers': [2],\n",
    "    'conv_filters_list': [[16, 32]],\n",
    "    'kernel_sizes_list': [[(3,3), (3,3)]],\n",
    "    'pooling_type': ['max'],\n",
    "    'dropout_rate': [0.2],\n",
    "    'dense_units': [32],\n",
    "    'activation': ['relu'],\n",
    "    'latent_dim': [8]\n",
    "}\n",
    "\n",
    "search = AutoMLSearch(\n",
    "    model_builder=cnn_autoencoder_builder,\n",
    "    param_grid=param_grid,\n",
    "    input_shape=(28,28,1),\n",
    "    epochs=1\n",
    ")\n",
    "\n",
    "search.fit(X_train, X_val)\n",
    "\n",
    "assert search.get_best_model() is not None\n",
    "assert search.get_best_params() is not None\n",
    "\n",
    "filters = search.extract_filters()\n",
    "assert len(filters) > 0\n",
    "\n",
    "print(\"✅ All tests passed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def print_filters(filters):\n",
    "    for idx, f in enumerate(filters):\n",
    "        print(f\"\\nLayer {idx+1} filters shape: {f.shape}\")\n",
    "        for i in range(f.shape[-1]):\n",
    "            kernel = f[..., i]\n",
    "            print(f\"\\nFilter {i+1} weights:\\n{kernel}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
